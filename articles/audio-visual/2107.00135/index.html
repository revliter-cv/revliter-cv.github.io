<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<link rel=icon href=/favicon.ico>
<title>
(NeurIPS 2021) Attention Bottleneck for Multimodal Fusion - Computer Vision Paper Reading
</title>
<meta name=description content="Conclusion (1) 2022.9:
关于特征融合，一般的处理方式是做fusion，在之前读视频理解相关的论文也提到，对于时空两个方向提取出的特征做fusion，包括有late fusion， early fusion，以及模型内部很多次的mid fusion，对于两个模态的特征融合一般主导方法是late fusion（主要是基于模型复杂度和精度的综合考虑），而google research发表了关于attention bottleneck的方法来改进时间复杂度，并且在精度上也很不错。相当于把(m+n)^2 的复杂度变成了(m+x)^2 + (n+x)^2，从而把两个模态特征大小的乘积项搞掉了。假设m=n的话其实就是近乎提高了一倍的效率。
关于fsn的部分和学长@YJG聊了一些，我个人觉得fsn不像cls一样直接出现在loss里面能够得到一个很强的监督信号来强制让他去整合单个模态的整体特征，而学长认为loss的传递会训练到fsn的部分，由此会产生监督效果, 并觉得现在网络的训练能力足够强大，不用担心这一块学习不到特征，而且实验结果确实能够证实这一点，整个模型确实表现的比单个模态的模型要好。这个说法我也感觉没什么大问题，但是还是感觉有一点奇怪，并固执地觉得或许只是在某一个实验的结果很好，只是故事讲的很好但是模型的效果不够solid，所以需要再做一些实验来证实这样的想法到底是不是一个很好的trick，（文章具体实验的部分也没有太仔细看，回头带着这样的想法再确认一下好了）。然后在晚上洗完澡后发呆的时候有了一些奇怪的想法，觉得fsn token其实和cls token一样，那我们其实是不是可以直接把fsn扔掉，然后给每个模态多加几个cls token，这样cls token都可以得到分类loss的强监督，再在模型中间用几个cls token之间做fusion或者说self-attention，最后得到多个cls token后加pooling和mlp就可以了。然后在和学长讨论了一下后感觉其实这个和fsn没有很大的区别，最后pooling加mlp的方式也是现在很多改进ViT的方法用的trick，不过倒是提供了一种可以小改fsn或者说解释fsn的方法。
(2) 2022.10:
关于后续能做的事情：加入FSN的结果就相当于扔掉了原来两个模态间mn级别的cross attention，在复杂度上面已经很难有所提升了，所以如果想在基于transformer的多模态的压缩加速的话，思路基本上就只能是(1)利用模态间的信息去做token dropping，或者说token selection (2)做一些decision unit来确定主导模态，这样或许能再砍一半 (3)就只能去搞单个模态的压缩加速，但是感觉和多模态就没什么关系了。
其中(1)的想法感觉有可以做的地方，还没有尝试，接下来需要再调研调研。
Implementation Details  TODO
 "><meta name=generator content="Hugo 0.92.2">
<link rel=stylesheet href=https://revliter-cv.github.io/css/main.css>
<meta property="og:title" content="(NeurIPS 2021) Attention Bottleneck for Multimodal Fusion">
<meta property="og:description" content="Conclusion (1) 2022.9:
关于特征融合，一般的处理方式是做fusion，在之前读视频理解相关的论文也提到，对于时空两个方向提取出的特征做fusion，包括有late fusion， early fusion，以及模型内部很多次的mid fusion，对于两个模态的特征融合一般主导方法是late fusion（主要是基于模型复杂度和精度的综合考虑），而google research发表了关于attention bottleneck的方法来改进时间复杂度，并且在精度上也很不错。相当于把(m+n)^2 的复杂度变成了(m+x)^2 + (n+x)^2，从而把两个模态特征大小的乘积项搞掉了。假设m=n的话其实就是近乎提高了一倍的效率。
关于fsn的部分和学长@YJG聊了一些，我个人觉得fsn不像cls一样直接出现在loss里面能够得到一个很强的监督信号来强制让他去整合单个模态的整体特征，而学长认为loss的传递会训练到fsn的部分，由此会产生监督效果, 并觉得现在网络的训练能力足够强大，不用担心这一块学习不到特征，而且实验结果确实能够证实这一点，整个模型确实表现的比单个模态的模型要好。这个说法我也感觉没什么大问题，但是还是感觉有一点奇怪，并固执地觉得或许只是在某一个实验的结果很好，只是故事讲的很好但是模型的效果不够solid，所以需要再做一些实验来证实这样的想法到底是不是一个很好的trick，（文章具体实验的部分也没有太仔细看，回头带着这样的想法再确认一下好了）。然后在晚上洗完澡后发呆的时候有了一些奇怪的想法，觉得fsn token其实和cls token一样，那我们其实是不是可以直接把fsn扔掉，然后给每个模态多加几个cls token，这样cls token都可以得到分类loss的强监督，再在模型中间用几个cls token之间做fusion或者说self-attention，最后得到多个cls token后加pooling和mlp就可以了。然后在和学长讨论了一下后感觉其实这个和fsn没有很大的区别，最后pooling加mlp的方式也是现在很多改进ViT的方法用的trick，不过倒是提供了一种可以小改fsn或者说解释fsn的方法。
(2) 2022.10:
关于后续能做的事情：加入FSN的结果就相当于扔掉了原来两个模态间mn级别的cross attention，在复杂度上面已经很难有所提升了，所以如果想在基于transformer的多模态的压缩加速的话，思路基本上就只能是(1)利用模态间的信息去做token dropping，或者说token selection (2)做一些decision unit来确定主导模态，这样或许能再砍一半 (3)就只能去搞单个模态的压缩加速，但是感觉和多模态就没什么关系了。
其中(1)的想法感觉有可以做的地方，还没有尝试，接下来需要再调研调研。
Implementation Details  TODO
 ">
<meta property="og:type" content="article">
<meta property="og:url" content="https://revliter-cv.github.io/articles/audio-visual/2107.00135/"><meta property="og:image" content="https://revliter-cv.github.io/digital-garden-logo.png"><meta property="article:section" content="articles">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://revliter-cv.github.io/digital-garden-logo.png">
<meta name=twitter:title content="(NeurIPS 2021) Attention Bottleneck for Multimodal Fusion">
<meta name=twitter:description content="Conclusion (1) 2022.9:
关于特征融合，一般的处理方式是做fusion，在之前读视频理解相关的论文也提到，对于时空两个方向提取出的特征做fusion，包括有late fusion， early fusion，以及模型内部很多次的mid fusion，对于两个模态的特征融合一般主导方法是late fusion（主要是基于模型复杂度和精度的综合考虑），而google research发表了关于attention bottleneck的方法来改进时间复杂度，并且在精度上也很不错。相当于把(m+n)^2 的复杂度变成了(m+x)^2 + (n+x)^2，从而把两个模态特征大小的乘积项搞掉了。假设m=n的话其实就是近乎提高了一倍的效率。
关于fsn的部分和学长@YJG聊了一些，我个人觉得fsn不像cls一样直接出现在loss里面能够得到一个很强的监督信号来强制让他去整合单个模态的整体特征，而学长认为loss的传递会训练到fsn的部分，由此会产生监督效果, 并觉得现在网络的训练能力足够强大，不用担心这一块学习不到特征，而且实验结果确实能够证实这一点，整个模型确实表现的比单个模态的模型要好。这个说法我也感觉没什么大问题，但是还是感觉有一点奇怪，并固执地觉得或许只是在某一个实验的结果很好，只是故事讲的很好但是模型的效果不够solid，所以需要再做一些实验来证实这样的想法到底是不是一个很好的trick，（文章具体实验的部分也没有太仔细看，回头带着这样的想法再确认一下好了）。然后在晚上洗完澡后发呆的时候有了一些奇怪的想法，觉得fsn token其实和cls token一样，那我们其实是不是可以直接把fsn扔掉，然后给每个模态多加几个cls token，这样cls token都可以得到分类loss的强监督，再在模型中间用几个cls token之间做fusion或者说self-attention，最后得到多个cls token后加pooling和mlp就可以了。然后在和学长讨论了一下后感觉其实这个和fsn没有很大的区别，最后pooling加mlp的方式也是现在很多改进ViT的方法用的trick，不过倒是提供了一种可以小改fsn或者说解释fsn的方法。
(2) 2022.10:
关于后续能做的事情：加入FSN的结果就相当于扔掉了原来两个模态间mn级别的cross attention，在复杂度上面已经很难有所提升了，所以如果想在基于transformer的多模态的压缩加速的话，思路基本上就只能是(1)利用模态间的信息去做token dropping，或者说token selection (2)做一些decision unit来确定主导模态，这样或许能再砍一半 (3)就只能去搞单个模态的压缩加速，但是感觉和多模态就没什么关系了。
其中(1)的想法感觉有可以做的地方，还没有尝试，接下来需要再调研调研。
Implementation Details  TODO
 ">
<meta itemprop=name content="(NeurIPS 2021) Attention Bottleneck for Multimodal Fusion">
<meta itemprop=description content="Conclusion (1) 2022.9:
关于特征融合，一般的处理方式是做fusion，在之前读视频理解相关的论文也提到，对于时空两个方向提取出的特征做fusion，包括有late fusion， early fusion，以及模型内部很多次的mid fusion，对于两个模态的特征融合一般主导方法是late fusion（主要是基于模型复杂度和精度的综合考虑），而google research发表了关于attention bottleneck的方法来改进时间复杂度，并且在精度上也很不错。相当于把(m+n)^2 的复杂度变成了(m+x)^2 + (n+x)^2，从而把两个模态特征大小的乘积项搞掉了。假设m=n的话其实就是近乎提高了一倍的效率。
关于fsn的部分和学长@YJG聊了一些，我个人觉得fsn不像cls一样直接出现在loss里面能够得到一个很强的监督信号来强制让他去整合单个模态的整体特征，而学长认为loss的传递会训练到fsn的部分，由此会产生监督效果, 并觉得现在网络的训练能力足够强大，不用担心这一块学习不到特征，而且实验结果确实能够证实这一点，整个模型确实表现的比单个模态的模型要好。这个说法我也感觉没什么大问题，但是还是感觉有一点奇怪，并固执地觉得或许只是在某一个实验的结果很好，只是故事讲的很好但是模型的效果不够solid，所以需要再做一些实验来证实这样的想法到底是不是一个很好的trick，（文章具体实验的部分也没有太仔细看，回头带着这样的想法再确认一下好了）。然后在晚上洗完澡后发呆的时候有了一些奇怪的想法，觉得fsn token其实和cls token一样，那我们其实是不是可以直接把fsn扔掉，然后给每个模态多加几个cls token，这样cls token都可以得到分类loss的强监督，再在模型中间用几个cls token之间做fusion或者说self-attention，最后得到多个cls token后加pooling和mlp就可以了。然后在和学长讨论了一下后感觉其实这个和fsn没有很大的区别，最后pooling加mlp的方式也是现在很多改进ViT的方法用的trick，不过倒是提供了一种可以小改fsn或者说解释fsn的方法。
(2) 2022.10:
关于后续能做的事情：加入FSN的结果就相当于扔掉了原来两个模态间mn级别的cross attention，在复杂度上面已经很难有所提升了，所以如果想在基于transformer的多模态的压缩加速的话，思路基本上就只能是(1)利用模态间的信息去做token dropping，或者说token selection (2)做一些decision unit来确定主导模态，这样或许能再砍一半 (3)就只能去搞单个模态的压缩加速，但是感觉和多模态就没什么关系了。
其中(1)的想法感觉有可以做的地方，还没有尝试，接下来需要再调研调研。
Implementation Details  TODO
 ">
<meta itemprop=wordCount content="35"><meta itemprop=image content="https://revliter-cv.github.io/digital-garden-logo.png">
<meta itemprop=keywords content>
<meta itemprop=name content="(NeurIPS 2021) Attention Bottleneck for Multimodal Fusion">
<meta itemprop=description content="Conclusion (1) 2022.9:
关于特征融合，一般的处理方式是做fusion，在之前读视频理解相关的论文也提到，对于时空两个方向提取出的特征做fusion，包括有late fusion， early fusion，以及模型内部很多次的mid fusion，对于两个模态的特征融合一般主导方法是late fusion（主要是基于模型复杂度和精度的综合考虑），而google research发表了关于attention bottleneck的方法来改进时间复杂度，并且在精度上也很不错。相当于把(m+n)^2 的复杂度变成了(m+x)^2 + (n+x)^2，从而把两个模态特征大小的乘积项搞掉了。假设m=n的话其实就是近乎提高了一倍的效率。
关于fsn的部分和学长@YJG聊了一些，我个人觉得fsn不像cls一样直接出现在loss里面能够得到一个很强的监督信号来强制让他去整合单个模态的整体特征，而学长认为loss的传递会训练到fsn的部分，由此会产生监督效果, 并觉得现在网络的训练能力足够强大，不用担心这一块学习不到特征，而且实验结果确实能够证实这一点，整个模型确实表现的比单个模态的模型要好。这个说法我也感觉没什么大问题，但是还是感觉有一点奇怪，并固执地觉得或许只是在某一个实验的结果很好，只是故事讲的很好但是模型的效果不够solid，所以需要再做一些实验来证实这样的想法到底是不是一个很好的trick，（文章具体实验的部分也没有太仔细看，回头带着这样的想法再确认一下好了）。然后在晚上洗完澡后发呆的时候有了一些奇怪的想法，觉得fsn token其实和cls token一样，那我们其实是不是可以直接把fsn扔掉，然后给每个模态多加几个cls token，这样cls token都可以得到分类loss的强监督，再在模型中间用几个cls token之间做fusion或者说self-attention，最后得到多个cls token后加pooling和mlp就可以了。然后在和学长讨论了一下后感觉其实这个和fsn没有很大的区别，最后pooling加mlp的方式也是现在很多改进ViT的方法用的trick，不过倒是提供了一种可以小改fsn或者说解释fsn的方法。
(2) 2022.10:
关于后续能做的事情：加入FSN的结果就相当于扔掉了原来两个模态间mn级别的cross attention，在复杂度上面已经很难有所提升了，所以如果想在基于transformer的多模态的压缩加速的话，思路基本上就只能是(1)利用模态间的信息去做token dropping，或者说token selection (2)做一些decision unit来确定主导模态，这样或许能再砍一半 (3)就只能去搞单个模态的压缩加速，但是感觉和多模态就没什么关系了。
其中(1)的想法感觉有可以做的地方，还没有尝试，接下来需要再调研调研。
Implementation Details  TODO
 ">
<meta itemprop=wordCount content="35"><meta itemprop=image content="https://revliter-cv.github.io/digital-garden-logo.png">
<meta itemprop=keywords content>
</head><body class="flex relative h-full min-h-screen"><aside class="will-change-transform transform transition-transform -translate-x-full absolute top-0 left-0 md:relative md:translate-x-0 w-3/4 md:w-60 h-full min-h-screen p-3 bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 flex flex-col gap-2.5 z-20 sidebar">
<p class="font-bold mb-5 flex items-center gap-2">
<button aria-label="Close sidebar" class="md:hidden menu-trigger-close p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-6 w-6" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></button>
<a href=https://revliter-cv.github.io/ class=px-2>
<span>Computer Vision Paper Reading</span>
</a>
<button aria-label="Toggle dark mode" class="dark-mode-toggle p-2 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="4"/><path d="M3 12h1m8-9v1m8 8h1m-9 8v1M5.6 5.6l.7.7m12.1-.7-.7.7m0 11.4.7.7M6.3 17.7l-.7.7"/></svg></button>
</p>
<ul class="list-none flex flex-col gap-1">
<li>
<a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between hover:bg-slate-200 dark:hover:bg-slate-700" href=/>
<span>Home</span>
</a>
</li>
<li>
<a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between text-slate-400 font-semibold pb-0 pl-1 border-b cursor-default pointer-events-none" href=#>
<span>Content</span>
</a>
</li>
<li>
<a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between hover:bg-slate-200 dark:hover:bg-slate-700" href=/articles>
<span>Articles</span>
</a>
</li>
<li>
<a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between hover:bg-slate-200 dark:hover:bg-slate-700" href=/works>
<span>Works done</span>
</a>
</li>
<li>
<a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between hover:bg-slate-200 dark:hover:bg-slate-700" href=/framework>
<span>Frameworks</span>
</a>
</li>
<li>
<a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between text-slate-400 font-semibold pb-0 pl-1 border-b cursor-default pointer-events-none" href=#>
<span>Links</span>
</a>
</li>
</ul>
<div class=flex-1></div>
<ul class="list-none flex flex-wrap justify-center gap-1 pt-2 border-t border-slate-200 dark:border-slate-600">
<li>
<a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700" href=https://github.com/revliter target=_blank rel="noopener noreferrer">
<span class=sr-only>GitHub</span>
<span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></span>
</a>
</li>
<li>
<a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700" href=mailto:201250019@smail.nju.edu.cn target=_blank rel="noopener noreferrer">
<span class=sr-only>Email</span>
<span><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="5" width="18" height="14" rx="2"/><polyline points="3 7 12 13 21 7"/></svg></span>
</a>
</li>
</ul>
</aside>
<div class="fixed bg-slate-700 bg-opacity-5 transition duration-200 ease-in-out inset-0 z-10 pointer-events-auto md:hidden left-0 top-0 w-full h-full hidden menu-overlay">
</div>
<button aria-label="Toggle Sidebar" class="md:hidden absolute top-3 left-3 z-10 menu-trigger p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-100"><svg class="h-6 w-6" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="4" y1="6" x2="20" y2="6"/><line x1="4" y1="12" x2="20" y2="12"/><line x1="4" y1="18" x2="16" y2="18"/></svg></button>
<div class=flex-1>
<script type=text/javascript async src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['[[',']]']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'})</script>
<style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style>
<div class="overflow-y-auto h-screen max-w-full">
<article class="left-3 px-6 py-20 max-w-full mx-10 prose lg:prose-lg h-fit dark:prose-invert prose-img:mx-auto">
<h1 class=!mb-2>(NeurIPS 2021) Attention Bottleneck for Multimodal Fusion</h1>
<p><img src=/images/AV/FSN1.png alt=FSN1>
<img src=/images/AV/FSN2.png alt=FSN2></p>
<h3 id=conclusion>Conclusion</h3>
<p>(1) 2022.9:</p>
<p>关于特征融合，一般的处理方式是做fusion，在之前读视频理解相关的论文也提到，对于时空两个方向提取出的特征做fusion，包括有late fusion， early fusion，以及模型内部很多次的mid fusion，对于两个模态的特征融合一般主导方法是late fusion（主要是基于模型复杂度和精度的综合考虑），而google research发表了关于attention bottleneck的方法来改进时间复杂度，并且在精度上也很不错。相当于把(m+n)^2 的复杂度变成了(m+x)^2 + (n+x)^2，从而把两个模态特征大小的乘积项搞掉了。假设m=n的话其实就是近乎提高了一倍的效率。</p>
<p>关于fsn的部分和学长@YJG聊了一些，我个人觉得fsn不像cls一样直接出现在loss里面能够得到一个很强的监督信号来强制让他去整合单个模态的整体特征，而学长认为loss的传递会训练到fsn的部分，由此会产生监督效果, 并觉得现在网络的训练能力足够强大，不用担心这一块学习不到特征，而且实验结果确实能够证实这一点，整个模型确实表现的比单个模态的模型要好。这个说法我也感觉没什么大问题，但是还是感觉有一点奇怪，并固执地觉得或许只是在某一个实验的结果很好，只是故事讲的很好但是模型的效果不够solid，所以需要再做一些实验来证实这样的想法到底是不是一个很好的trick，（文章具体实验的部分也没有太仔细看，回头带着这样的想法再确认一下好了）。然后在晚上洗完澡后发呆的时候有了一些奇怪的想法，觉得fsn token其实和cls token一样，那我们其实是不是可以直接把fsn扔掉，然后给每个模态多加几个cls token，这样cls token都可以得到分类loss的强监督，再在模型中间用几个cls token之间做fusion或者说self-attention，最后得到多个cls token后加pooling和mlp就可以了。然后在和学长讨论了一下后感觉其实这个和fsn没有很大的区别，最后pooling加mlp的方式也是现在很多改进ViT的方法用的trick，不过倒是提供了一种可以小改fsn或者说解释fsn的方法。</p>
<p>(2) 2022.10:</p>
<p>关于后续能做的事情：加入FSN的结果就相当于扔掉了原来两个模态间mn级别的cross attention，在复杂度上面已经很难有所提升了，所以如果想在基于transformer的多模态的压缩加速的话，思路基本上就只能是(1)利用模态间的信息去做token dropping，或者说token selection (2)做一些decision unit来确定主导模态，这样或许能再砍一半 (3)就只能去搞单个模态的压缩加速，但是感觉和多模态就没什么关系了。</p>
<p>其中(1)的想法感觉有可以做的地方，还没有尝试，接下来需要再调研调研。</p>
<h3 id=implementation-details>Implementation Details</h3>
<blockquote>
<p>TODO</p>
</blockquote>
</article>
</div>
</div>
<script type=text/javascript src=/main.js defer></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-PROPERTY_ID','auto'),ga('send','pageview'))</script>
</body>
</html>