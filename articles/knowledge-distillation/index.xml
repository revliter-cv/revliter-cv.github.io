<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Knowledge Distillation on Computer Vision Paper Reading</title><link>https://revliter-cv.github.io/articles/knowledge-distillation/</link><description>Recent content in Knowledge Distillation on Computer Vision Paper Reading</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://revliter-cv.github.io/articles/knowledge-distillation/index.xml" rel="self" type="application/rss+xml"/><item><title>(CVPR) A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning</title><link>https://revliter-cv.github.io/articles/knowledge-distillation/fsp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/knowledge-distillation/fsp/</guid><description>这篇文章的主要想法是，teacher网络不是直接告诉student一个结果让他自己学，而是对于student网络的知识flow进行监督。
We define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers
然后提出了一种FSP matrix的东西来解决这个问题
The extracted feature maps from two layers are used to generate the flow of solution procedure (FSP) matrix. The student DNN is trained to make its FSP matrix similar to that of the teacher DNN.
感觉这个文章提到的方法或许需要T和S的网络结构比较相似才适合这样学习？（）文章用来验证的方法是不同层数的residual module进行这种FSP matrix的对齐。</description></item><item><title>(NeurIPS 2014) Distilling the Knowledge in a Neural Network</title><link>https://revliter-cv.github.io/articles/knowledge-distillation/1503.02531/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/knowledge-distillation/1503.02531/</guid><description>arXiv: 1503.02531
Conclusion: 这篇论文算是KD领域的开山之作（？），作者是大名鼎鼎的Hinton老爷子，解决的主要问题是：目前的模型ensemble很有用，能显著提点，但是这会导致计算量太大而难以得到实际的应用，于是希望能把模型ensemble综合成一个模型，这样也能减小模型本身的variance，增加泛化能力。
然后关于蒸馏，基本上就是拿模型ensemble出来的结果作为监督目标训练子网络，而传统的通过softmax得到的结果可能因为太过‘hard’而导致学习效果不好，所以提出了一种带温度的softed-softmax，Using a higher value for T produces a softer probability distribution over classes. 然后如果用来训练的样本有标注，可以把标注的信息也作为一个监督信号。
关于这个方法的缺点呢，
Although the KD training achieved improved accuracy over several datasets, this method has limitations such as difficulty with optimizing very deep networks. Its effectiveness only limits to softmax loss function, and relies on the number of classes. For example, in a binary classifi- cation problem, KD could hardly improve the performance since almost no additional supervision could be provided.</description></item><item><title>Relational Knowledge Distillation</title><link>https://revliter-cv.github.io/articles/knowledge-distillation/1904.05068/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/knowledge-distillation/1904.05068/</guid><description/></item></channel></rss>