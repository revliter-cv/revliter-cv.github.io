<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Supression or Acceleration on Computer Vision Paper Reading</title><link>https://revliter-cv.github.io/articles/model-supression/</link><description>Recent content in Model Supression or Acceleration on Computer Vision Paper Reading</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://revliter-cv.github.io/articles/model-supression/index.xml" rel="self" type="application/rss+xml"/><item><title>(CVPR2022 Oral) A-ViT:Adaptive Tokens for Efficient Vision Transformer</title><link>https://revliter-cv.github.io/articles/model-supression/a-vit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/model-supression/a-vit/</guid><description>First update: 2022.10.21
This paper is published in arXiv later than the AdaVit talked before. So it just changes its name to A-Vit (My guess).
And like DynamicVit, this paper is also focused on token selection. So we start with the final result:
The A-Vit, as shown in these two tables, performs better than DynamicVit!
Spotlight: A-ViT requires no extra parameters or sub-network for halting improves the throughput of DeiT-Tiny by 62% and DeiT-Small by 38% with only 0.</description></item><item><title>(CVPR2022) AdaVit:Adaptive Vision Transformers for Efficient Image Recognition</title><link>https://revliter-cv.github.io/articles/model-supression/adavit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/model-supression/adavit/</guid><description>First update: 2022.10.19
Insight: we argue that due to the large variations among images, their need for modeling long-range dependencies between patches differ.
About adavit: an adaptive computation framework that learns to derive usage policies on which patches, self-attention heads and transformer blocks to use throughout the backbone on a per-input basis, aiming to improve inference efficiency of vision transformers with a minimal drop of accuracy for image recognition.</description></item><item><title>(NIPS2021) DynamicViT:Efficient Vision Transformers with Dynamic Token Sparsification</title><link>https://revliter-cv.github.io/articles/model-supression/dynamicvit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/model-supression/dynamicvit/</guid><description>First update: 2022.10.19
I just write the blog the same time I read it. So I will make it casual and may rewrite some content later.
This is the first paper I read about ViT&amp;rsquo;s acceleration.
The authors proposed in the abstract: we observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition.
HOW can we come to this conclusion?</description></item><item><title>MobileNets</title><link>https://revliter-cv.github.io/articles/model-supression/mobilenet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/model-supression/mobilenet/</guid><description/></item></channel></rss>