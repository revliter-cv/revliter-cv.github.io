<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Supression or Acceleration on Computer Vision Paper Reading</title><link>https://revliter-cv.github.io/articles/model-supression/</link><description>Recent content in Model Supression or Acceleration on Computer Vision Paper Reading</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://revliter-cv.github.io/articles/model-supression/index.xml" rel="self" type="application/rss+xml"/><item><title>DynamicViT:Efficient Vision Transformers with Dynamic Token Sparsification</title><link>https://revliter-cv.github.io/articles/model-supression/dynamicvit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/model-supression/dynamicvit/</guid><description>First update: 2022.10.19
I just write the blog the same time I read it. So I will make it casual and may rewrite some content later.
The authors proposed in the abstract: we observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition.
HOW can we come to this conclusion? Visualization:
About this visualization method: Paper related: Tranformer interpretability beyond attention visualization</description></item><item><title>MobileNets</title><link>https://revliter-cv.github.io/articles/model-supression/mobilenet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/model-supression/mobilenet/</guid><description/></item></channel></rss>