<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Homepage on Computer Vision Paper Reading</title><link>https://revliter-cv.github.io/</link><description>Recent content in Homepage on Computer Vision Paper Reading</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://revliter-cv.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>(CVPR) A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning</title><link>https://revliter-cv.github.io/articles/knowledge-distillation/fsp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/knowledge-distillation/fsp/</guid><description>这篇文章的主要想法是，teacher网络不是直接告诉student一个结果让他自己学，而是对于student网络的知识flow进行监督。
We define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers
然后提出了一种FSP matrix的东西来解决这个问题
The extracted feature maps from two layers are used to generate the flow of solution procedure (FSP) matrix. The student DNN is trained to make its FSP matrix similar to that of the teacher DNN.
感觉这个文章提到的方法或许需要T和S的网络结构比较相似才适合这样学习？（）文章用来验证的方法是不同层数的residual module进行这种FSP matrix的对齐。</description></item><item><title>(CVPR2020) Listen to Look:Action Recognition by Previewing Audio</title><link>https://revliter-cv.github.io/articles/audio-visual/1912.03387/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/audio-visual/1912.03387/</guid><description/></item><item><title>(I3D) Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title><link>https://revliter-cv.github.io/articles/video-understanding/i3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/video-understanding/i3d/</guid><description/></item><item><title>(ICLR 2022) BEIT:BERT PRE-TRAINING OF IMAGE TRANSFORMERS</title><link>https://revliter-cv.github.io/articles/ssl/beit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/ssl/beit/</guid><description/></item><item><title>(NeurIPS 2014) Distilling the Knowledge in a Neural Network</title><link>https://revliter-cv.github.io/articles/knowledge-distillation/1503.02531/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/knowledge-distillation/1503.02531/</guid><description>arXiv: 1503.02531
Conclusion: 这篇论文算是KD领域的开山之作（？），作者是大名鼎鼎的Hinton老爷子，解决的主要问题是：目前的模型ensemble很有用，能显著提点，但是这会导致计算量太大而难以得到实际的应用，于是希望能把模型ensemble综合成一个模型，这样也能减小模型本身的variance，增加泛化能力。
然后关于蒸馏，基本上就是拿模型ensemble出来的结果作为监督目标训练子网络，而传统的通过softmax得到的结果可能因为太过‘hard’而导致学习效果不好，所以提出了一种带温度的softed-softmax，Using a higher value for T produces a softer probability distribution over classes. 然后如果用来训练的样本有标注，可以把标注的信息也作为一个监督信号。
关于这个方法的缺点呢，
Although the KD training achieved improved accuracy over several datasets, this method has limitations such as difficulty with optimizing very deep networks. Its effectiveness only limits to softmax loss function, and relies on the number of classes. For example, in a binary classifi- cation problem, KD could hardly improve the performance since almost no additional supervision could be provided.</description></item><item><title>(NeurIPS 2021) Attention Bottleneck for Multimodal Fusion</title><link>https://revliter-cv.github.io/articles/audio-visual/2107.00135/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/audio-visual/2107.00135/</guid><description>Conclusion (1) 2022.9:
关于特征融合，一般的处理方式是做fusion，在之前读视频理解相关的论文也提到，对于时空两个方向提取出的特征做fusion，包括有late fusion， early fusion，以及模型内部很多次的mid fusion，对于两个模态的特征融合一般主导方法是late fusion（主要是基于模型复杂度和精度的综合考虑），而google research发表了关于attention bottleneck的方法来改进时间复杂度，并且在精度上也很不错。相当于把(m+n)^2 的复杂度变成了(m+x)^2 + (n+x)^2，从而把两个模态特征大小的乘积项搞掉了。假设m=n的话其实就是近乎提高了一倍的效率。
关于fsn的部分和学长@YJG聊了一些，我个人觉得fsn不像cls一样直接出现在loss里面能够得到一个很强的监督信号来强制让他去整合单个模态的整体特征，而学长认为loss的传递会训练到fsn的部分，由此会产生监督效果, 并觉得现在网络的训练能力足够强大，不用担心这一块学习不到特征，而且实验结果确实能够证实这一点，整个模型确实表现的比单个模态的模型要好。这个说法我也感觉没什么大问题，但是还是感觉有一点奇怪，并固执地觉得或许只是在某一个实验的结果很好，只是故事讲的很好但是模型的效果不够solid，所以需要再做一些实验来证实这样的想法到底是不是一个很好的trick，（文章具体实验的部分也没有太仔细看，回头带着这样的想法再确认一下好了）。然后在晚上洗完澡后发呆的时候有了一些奇怪的想法，觉得fsn token其实和cls token一样，那我们其实是不是可以直接把fsn扔掉，然后给每个模态多加几个cls token，这样cls token都可以得到分类loss的强监督，再在模型中间用几个cls token之间做fusion或者说self-attention，最后得到多个cls token后加pooling和mlp就可以了。然后在和学长讨论了一下后感觉其实这个和fsn没有很大的区别，最后pooling加mlp的方式也是现在很多改进ViT的方法用的trick，不过倒是提供了一种可以小改fsn或者说解释fsn的方法。
(2) 2022.10:
关于后续能做的事情：加入FSN的结果就相当于扔掉了原来两个模态间mn级别的cross attention，在复杂度上面已经很难有所提升了，所以如果想在基于transformer的多模态的压缩加速的话，思路基本上就只能是(1)利用模态间的信息去做token dropping，或者说token selection (2)做一些decision unit来确定主导模态，这样或许能再砍一半 (3)就只能去搞单个模态的压缩加速，但是感觉和多模态就没什么关系了。
其中(1)的想法感觉有可以做的地方，还没有尝试，接下来需要再调研调研。
Implementation Details TODO</description></item><item><title>(Slowfast) SlowFast Networks for Video Recognition</title><link>https://revliter-cv.github.io/articles/video-understanding/slowfast/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/video-understanding/slowfast/</guid><description/></item><item><title>(Tdd) Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</title><link>https://revliter-cv.github.io/articles/video-understanding/tdd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/video-understanding/tdd/</guid><description/></item><item><title>BERT</title><link>https://revliter-cv.github.io/articles/ssl/bert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/ssl/bert/</guid><description/></item><item><title>Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing</title><link>https://revliter-cv.github.io/articles/audio-visual/2204.11573/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/audio-visual/2204.11573/</guid><description/></item><item><title>Masked Autoencoders Are Scalable Vision Learners</title><link>https://revliter-cv.github.io/articles/ssl/mae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/ssl/mae/</guid><description/></item><item><title>MobileNets</title><link>https://revliter-cv.github.io/articles/model-supression/mobilenet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/model-supression/mobilenet/</guid><description/></item><item><title>MOCO</title><link>https://revliter-cv.github.io/articles/contrastive-learning/1911.05722/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/contrastive-learning/1911.05722/</guid><description/></item><item><title>MOCO v2</title><link>https://revliter-cv.github.io/articles/contrastive-learning/2003.04297/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/contrastive-learning/2003.04297/</guid><description/></item><item><title>Relational Knowledge Distillation</title><link>https://revliter-cv.github.io/articles/knowledge-distillation/1904.05068/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/knowledge-distillation/1904.05068/</guid><description/></item><item><title>VideoMAE:Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</title><link>https://revliter-cv.github.io/articles/ssl/videomae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://revliter-cv.github.io/articles/ssl/videomae/</guid><description/></item></channel></rss>